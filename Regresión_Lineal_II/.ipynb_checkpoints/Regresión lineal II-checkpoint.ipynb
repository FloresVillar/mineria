{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"uni-logo.png\" alt=\"Logo UNI\" width=150 hight=300 align=\"right\">\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "<h1><font color=\"#7F000E\" size=4>Minería de Datos (CC442)</font></h1>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#7F000E\" size=6>Regresión lineal II</font></h1>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#7F000E\" size=3>Yuri Coicca, M.Sc.</font><br>\n",
    "<font color=\"#7F000E\" size=3>Facultad de Ciencias</font><br>\n",
    "<font color=\"#7F000E\" size=3>Ciencia de la Computación - UNI</font><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#7F000E\"> Regresión lineal multivariante </font>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "Como vimos en el ejercicio anterior, la regresión lineal multivariada expresa la variable de salida como combinación lineal de las variables de entrada:\n",
    "\n",
    "\n",
    "$$\n",
    "y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots \\theta_k x_k\n",
    "$$\n",
    "\n",
    "\n",
    "El error asociado a un modelo $\\theta = ( \\theta_0,  \\theta_1 , \\ldots,  \\theta_k)$ con respecto a un conjunto de datos se expresa como:\n",
    "\n",
    "$$\n",
    "error(\\theta) = \\frac{1}{N}\\sum_{i=0}^N \\left(y^{(i)} - (\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} + \\cdots \\theta_k x_k^{(i)})\\right)^2    \n",
    "$$\n",
    "\n",
    "En el contexto de aprendizaje automático, se utiliza una función similar, denominada de *__coste__* o *__pérdida__*, que se formula como:\n",
    "\n",
    "$$\n",
    "coste(\\theta) = \\frac{1}{2N}\\sum_{i=0}^N \\left((\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} + \\cdots \\theta_k x_k^{(i)}) - y^{(i)}  \\right)^2    \n",
    "$$\n",
    "\n",
    "Por otra parte, en el tutorial anterior también se vió que una forma de obtener la combinación óptima de parámetros, la que minimiza la función de coste, es mediante la resolución de la ecuación normal:\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y\n",
    "$$\n",
    "\n",
    "\n",
    "<bR>\n",
    "Ambas funciones se implementan a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Calcula la función de coste\n",
    "def get_coste(X, Y, beta):\n",
    "    return np.sum((X.dot(beta)-Y)**2)/(2*len(Y))\n",
    "\n",
    "# Obtiene los valores de los coeficientes mediante la ecuación normal\n",
    "def get_lrmodel_ne(X,Y):\n",
    "    #return np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),Y)\n",
    "    #return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "    return np.linalg.inv(X.T @ X )@ X.T @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que la obtención de los coeficientes mediante la ecuación normal (la complejidad del cálculo de $(X^T \\cdot X)^{-1}$ es ${\\cal{O}}(N^3)$), en la práctica se utilizan otros algoritmos como ___gradiente descendiente___ que, debido a su importancia se va a ver en este curso. Para ello, se estudiarán las propiedades de la función de coste, la información que proporciona la derivada de esta función y, finalmente, cómo el algoritmo gradiente descendiente permite encontrar el punto mínimo de la función de coste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3><font color=\"#7F000E\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 1</font></h3>\n",
    "\n",
    "En el ejercicio anterior se trataron de manera independiente la regresión lineal univariada y la múltiple. Sin embargo, la regresión lineal univariada es un caso concreto de la múltiple. El archivo `auto.txt` contiene los datos de peso y consumo para un conjunto de coches. Utilizando las funciones `get_coste()` y `get_lrmodel_ne()`, obtener los coeficientes del modelo, dibujar la recta correspondiente, e imprimir el coste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo es:  [-0.715713    9.04047744]\n",
      "El coste del modelo es: 2.267\n"
     ]
    }
   ],
   "source": [
    "# Lectura de los datos\n",
    "datos = np.loadtxt('auto.txt')\n",
    "N,_ = datos.shape \n",
    "X = datos[:,0]\n",
    "Y = datos[:,1]\n",
    "\n",
    "# Añade el término independiente\n",
    "X.shape = (N,1)\n",
    "X = np.hstack((np.ones(X.shape), X))\n",
    "\n",
    "# Calcula el modelo \n",
    "theta = get_lrmodel_ne(X,Y)\n",
    "print(\"El modelo es: \", theta)\n",
    "\n",
    "# Calcula e imprime el coste\n",
    "coste = get_coste(X,Y,theta)\n",
    "print(\"El coste del modelo es: {:.3f}\".format(coste))\n",
    "\n",
    "# Para mostrar el modelo, genera valores relativos al peso\n",
    "X_modelo = np.arange(0.5, 2.5, 0.05)\n",
    "X_modelo.shape=(len(X_modelo),1)\n",
    "\n",
    "# Añade la columna de unos\n",
    "X_modelo = np.hstack((np.ones(X_modelo.shape), X_modelo))\n",
    "# Obtiene las salidas correspondientes\n",
    "Y_modelo = np.dot(X_modelo, theta)\n",
    "\n",
    "# Dibuja la gráfica \n",
    "plt.scatter(X[:,1],Y)\n",
    "# Dibuja la recta\n",
    "plt.plot(X_modelo[:,1], Y_modelo, 'r');\n",
    "plt.xlabel('Peso');\n",
    "plt.ylabel('Consumo');\n",
    "\n",
    "#El modelo es:  [-0.715713    9.04047744]\n",
    "#El coste del modelo es: 2.267"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#004D7F\"></i>\n",
    " </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## <font color=\"#004D7F\">La función de coste </font>\n",
    "\n",
    "Como se vio en la primera parte del tutorial, cada configuración de valores de $\\theta$ da lugar a una recta y, por consiguiente, a un valor de coste. En este ejemplo vamos a partir de una configuración (arbitraria) en la que $\\theta_0=0$ y $\\theta_1=5$, para ver cómo afecta la variación en los parámetros a la función de coste. \n",
    "\n",
    "<br>\n",
    "<h3><font color=\"#004D7F\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 2</font></h3>\n",
    "\n",
    "Calcular los costes para un conjunto de modelos en los que $\\theta_1 = 5$, y $\\theta_0$ tome valores en el rango $[-10, 10]$. Elaborar la gráfica ($\\theta_0$ / coste) correspondiente. Repetir la operación para $\\theta_1$, fijando $\\theta_0 = 0$ y considerando valores $\\theta_1$ en el rango $[4,14]$. \n",
    "\n",
    "¿Qué se puede apreciar en ellas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.443083   21.02322502 20.60801939 20.19746611 19.79156517]\n"
     ]
    }
   ],
   "source": [
    "# Fijamos theta_1\n",
    "theta_1 = 5\n",
    "# Crea los distintos valores para theta_0 y un array para almacenar costes correspondientes\n",
    "valores_0 = np.arange(-10,10,0.05)\n",
    "\n",
    "costes_0 = np.zeros(valores_0.shape)\n",
    "\n",
    "# Obtiene los valores y dibuja la figura\n",
    "for i, theta_0 in enumerate(valores_0):\n",
    "    costes_0[i] = get_coste(X,Y, (theta_0, theta_1))\n",
    "\n",
    "# Fijamos theta_0\n",
    "theta_0 = 0\n",
    "# Crea los distintos valores para theta_1 y un array para almacenar los costes correspondientes. \n",
    "valores_1 = np.arange(4,14,0.05)\n",
    "\n",
    "costes_1 = np.zeros(valores_1.shape)\n",
    "# Obtiene los valores y dibuja la figura\n",
    "for i, theta_1 in enumerate(valores_1):\n",
    "    costes_1[i] = get_coste(X,Y, (theta_0, theta_1))\n",
    "    \n",
    "# Crea una figura en la que va a incluir 2 gráficas, distribuídas en una rejilla de 1x2\n",
    "f, axes_grid = plt.subplots(1, 2)\n",
    "axes_grid[0].plot(valores_0, costes_0, 'r');    \n",
    "axes_grid[0].set_xlabel(r'$\\theta_0$');\n",
    "axes_grid[0].set_ylabel(r'$Coste(\\theta)$');\n",
    "axes_grid[1].plot(valores_1, costes_1, 'r');\n",
    "axes_grid[1].set_xlabel(r'$\\theta_1$');\n",
    "\n",
    "print(costes_1[:5])\n",
    "#[21.443083   21.02322502 20.60801939 20.19746611 19.79156517]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> \n",
    "  Se aprecia que las funciones son convexas, es decir, la tangente a la función siempre está por debajo de la misma, y el segmento que une dos puntos, está por encima. Estas funciones no tienen máximos (sí en los extremos del rango) y tienen ___un solo mínimo___. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#004D7F\"></i>\n",
    " </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3><font color=\"#004D7F\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 3</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el archivo `bodyfat.txt` y considerar el modelo en el que $\\theta_j=0 \\,\\, \\forall j$. Obtener una gráfica similar a las anteriores a partir del coeficiente $\\theta_3$, considerando para éste el rango $[-2,2]$  (gráfica $\\theta_3$ vs coste). ¿Qué se observa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6153.7580754  5898.15320933 5647.98043651 5403.23975694 5163.93117063]\n"
     ]
    }
   ],
   "source": [
    "# Carga los datos\n",
    "datos = np.loadtxt('bodyfat.txt')\n",
    "N,k = datos.shape\n",
    "k=k-1\n",
    "#print(N,\" casos, \",k,\" variables\")\n",
    "\n",
    "# Obtiene las variables de entrada y de salida.\n",
    "X = datos[:,:-1]\n",
    "Y = datos[:,-1]\n",
    "\n",
    "# Añade una columna con todos los valores a 1.\n",
    "X = np.hstack((np.ones((N,1)), X))\n",
    "\n",
    "# Calcula el modelo \n",
    "theta = np.zeros(15,)\n",
    "\n",
    "# Crea los distintos valores para theta_3 y un array\n",
    "# para almacenar costes.\n",
    "valores_3= np.arange(-2,2,0.05)\n",
    "coste_3 = np.zeros(valores_3.shape)\n",
    "\n",
    "# Obtiene los valores y dibuja la figura\n",
    "for i, theta_3 in enumerate(valores_3):\n",
    "    theta[3] = theta_3\n",
    "    coste_3[i] = get_coste(X,Y,theta) \n",
    "    \n",
    "plt.plot(valores_3, coste_3, 'r')\n",
    "plt.xlabel(r'$\\theta_3$');\n",
    "plt.ylabel(r'$Coste(\\theta)$');\n",
    "\n",
    "print(coste_3[:5])\n",
    "#[6153.7580754  5898.15320933 5647.98043651 5403.23975694 5163.93117063]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> \n",
    "    Igual que anteriormente, la función de coste con respecto al valor de $\\theta_3$ es convexa. Puede comprobarse que esto pasa para todos los parámetros, es decir, ___la función de coste es convexa___.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#004D7F\"></i>\n",
    " </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## <font color=\"#004D7F\"> Gradientes </font>\n",
    "\n",
    "<h3><font color=\"#004D7F\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 4</font></h3>\n",
    "\n",
    "El valor de la derivada  de una función (también las derivadas parciales) refleja la pendiente de la recta tangente a la función en ese punto.  Debido a que la función $coste(\\theta)$, es convexa, solamente existe un ___único punto en el que la derivada es cero___, que corresponde con el mínimo de la función.\n",
    "\n",
    "<br>\n",
    "La derivada parcial de la función de coste respecto a un parámetro, $\\theta_j$,  se calcula como:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}coste(\\theta) = \\frac{1}{N} \\left[ \\sum_{i=0}^N \\left((\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} + \\cdots \\theta_k x_k^{(i)})- y^{(i)} \\right) \\cdot x_j^{(i)} \\right] \n",
    "$$\n",
    "\n",
    "<br>\n",
    "Calcular el valor de la derivada parcial $\\frac{\\partial}{\\partial \\theta_3}coste(\\theta)$ en el modelo en el que $\\theta_j=0 \\,\\, \\forall j$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> \n",
    " El valor de la derivada parcial se puede calcular en una sola expresión. En este caso devuelve un valor muy cercano a cero.\n",
    "</div>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de θ₃ es: 0.000000.\n",
      "El valor de la derivada parcial para θ₃ en ese punto es: -820.743651.\n"
     ]
    }
   ],
   "source": [
    "# Calcula el modelo óptimo\n",
    "theta =np.zeros(15,)\n",
    "\n",
    "j=3\n",
    "gradiente = np.sum(((X @ theta)-Y) * X[:,j])/N\n",
    "print(\"El valor de \\u03B8\\u2083 es: {:6f}.\".format(theta[j]))\n",
    "print(\"El valor de la derivada parcial para \\u03B8\\u2083 en ese punto es: {:.6f}.\".format(gradiente))\n",
    "\n",
    "#El valor de θ₃ es: 0.000000.\n",
    "#El valor de la derivada parcial para θ₃ en ese punto es: -820.743651."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "&nbsp;El valor de la derivada parcial de la función de coste respecto a un coeficiente $\\theta_j$, consiste en: calcular el error del modelo para cada uno de los ejemplos $i$ en la base de datos; multiplicar el error correspondiente a cada ejemplo $i$ por el valor de la variable $j$ en el ejemplo $i$; y finalmente sumar todos los productos y dividirlos por el número de ejemplos. \n",
    "\n",
    "\n",
    "<br><br>\n",
    "En el caso de $j=0$, como $x_0^{(i)}=1 \\, \\forall i$, la derivada parcial es la media de los errores.\n",
    "    \n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_0}coste(\\theta) = \\frac{1}{N} \\left[ \\sum_{i=0}^N \\left((\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} + \\cdots \\theta_k x_k^{(i)})- y^{(i)} \\right) \\cdot 1 \\right] = \\frac{1}{N} \\left[ \\sum_{i=0}^N (\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} + \\cdots \\theta_k x_k^{(i)})- y^{(i)}  \\right]\n",
    "$$    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Calcular el valor de la derivada parcial en $\\theta_3$ para los valores $\\{0, 0.1, 0.2, 0.3, 0.32, 0.34, 0.36, 0.37, 0.38, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\\} $ (el resto de valores de $\\theta$ permanecen fijos a cero).\n",
    "- ¿Qué información nos proporciona el signo de la derivada parcial en un punto con respecto a la posición del valor óptimo $\\theta_3$?\n",
    "- ¿Qué relación tiene el valor absoluto de la derivada en un punto con la distancia de ese punto al mínimo? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de la derivada parcial para θ₃ en 0.0 es:  -820.7436507936508\n",
      "El valor de la derivada parcial para θ₃ en 0.1 es:  -603.4599206349208\n",
      "El valor de la derivada parcial para θ₃ en 0.2 es:  -386.17619047619047\n",
      "El valor de la derivada parcial para θ₃ en 0.3 es:  -168.89246031746035\n",
      "El valor de la derivada parcial para θ₃ en 0.32 es:  -125.43571428571427\n",
      "El valor de la derivada parcial para θ₃ en 0.34 es:  -81.97896825396822\n",
      "El valor de la derivada parcial para θ₃ en 0.36 es:  -38.522222222222254\n",
      "El valor de la derivada parcial para θ₃ en 0.37 es:  -16.7938492063492\n",
      "El valor de la derivada parcial para θ₃ en 0.38 es:  4.934523809523815\n",
      "El valor de la derivada parcial para θ₃ en 0.4 es:  48.391269841269896\n",
      "El valor de la derivada parcial para θ₃ en 0.5 es:  265.675\n",
      "El valor de la derivada parcial para θ₃ en 0.6 es:  482.95873015873013\n",
      "El valor de la derivada parcial para θ₃ en 0.7 es:  700.2424603174602\n",
      "El valor de la derivada parcial para θ₃ en 0.8 es:  917.5261904761907\n",
      "El valor de la derivada parcial para θ₃ en 0.9 es:  1134.8099206349204\n",
      "El valor de la derivada parcial para θ₃ en 1.0 es:  1352.0936507936508\n"
     ]
    }
   ],
   "source": [
    "valores_theta = np.array([0, 0.1, 0.2, 0.3, 0.32, 0.34, 0.36, 0.37, 0.38, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "for valor_theta in valores_theta:\n",
    "    theta[3] = valor_theta\n",
    "    gradiente = np.sum(((X @ theta)-Y) * X[:,j])/N\n",
    "    print(\"El valor de la derivada parcial para \\u03B8\\u2083 en\", valor_theta,  \"es: \", gradiente)\n",
    "    \n",
    "#El valor de la derivada parcial para θ₃ en 0.0 es:  -820.7436507936509\n",
    "#El valor de la derivada parcial para θ₃ en 0.1 es:  -603.459920634921\n",
    "#El valor de la derivada parcial para θ₃ en 0.2 es:  -386.1761904761904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "&nbsp; Se aprecian dos cosas (mirar también la función generada en el ejercicio 3):\n",
    "\n",
    "<br><br>\n",
    "\n",
    "1. Cuando la derivada parcial para  $\\theta_3$ en un punto es negativa, esto quiere decir que la función decrece. Por tanto, el valor óptimo de $\\theta_3$ es mayor que ese punto. Por ejemplo, la derivada parcial para $\\theta_3=-0$ es $-820.74$. Eso quiere decir que el valor óptimo de $\\theta_3$ está a la derecha, es decir, es mayor que $-0$. Y viceversa, si la derivada parcial en un punto es positiva, quiere decir que el el valor óptimo de $\\theta_3$ es menor que ese punto.\n",
    "    \n",
    "<br><br>\n",
    "2.  Debido a la forma de la función, cuanto más cerca del valor óptimo nos encontremos, más despacio decrece la función. \n",
    "\n",
    "<br><br>\n",
    "Es decir, la derivada parcial en un punto nos proporciona información sobre ___dónde está el valor óptimo (izquierda o derecha), y a qué distancia___. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#004D7F\"></i>\n",
    " </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3><font color=\"#004D7F\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 5</font></h3>\n",
    "\n",
    "Fijar $\\theta_7=10$ (el resto de parámetros permanece a cero), y calcular  $\\frac{\\partial}{\\partial \\theta_7}coste(\\theta)$. ¿El valor óptimo para $\\theta_7$ es mayor o es menor que 10? ¿Está muy lejos de 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de la derivada parcial para θ₇ en ese punto es: 100519.94\n"
     ]
    }
   ],
   "source": [
    "# Obtiene los coeficientes (puede haber cambiado el valor de theta en una celda anterior)\n",
    "theta =np.zeros(15,)\n",
    "# Se cambia el valor de θ₇ a 10\n",
    "theta[7]=10\n",
    "# Calcula el gradiente\n",
    "gradiente = np.sum(((X @ theta)-Y) * X[:,7])/N\n",
    "print(\"El valor de la derivada parcial para \\u03B8\\u2087 en ese punto es: {:.2f}\".format(gradiente))\n",
    "\n",
    "#El valor de la derivada parcial para θ₇ en ese punto es: 100519.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "&nbsp;  El valor es positivo, por lo tanto, el valor óptimo para $\\theta_7$ ha de ser menor de 10. También se puede ver que el valor es muy alto, por lo que el óptimo está lejos del óptimo (aunque no podemos decir cuanto).\n",
    "    \n",
    "    Un ejercicio interesante es ir cambiando manualmente el valor de $\\theta_7$, en función de la derivada, hasta acercarnos al óptimo. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#004D7F\"></i>\n",
    " </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3><font color=\"#004D7F\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 6</font></h3>\n",
    "\n",
    "Dada una configuración de valores $\\theta$ anterior (con $\\theta_7=10$) definir una función, denominada `gradiente()` que devuelva las derivadas parciales para cada $\\theta$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> \n",
    "El cuerpo de la función se puede implementar en una sola línea, aunque se puede considerar un truco de implementación.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   990.01230159,   1043.94597798,  19450.57309524,  44620.83968254,\n",
       "       179324.11406746,  69488.57619048,  37771.13047619, 100519.9381746 ,\n",
       "        92454.55920635,  99401.69781746,  59131.64678571,  38349.26809524,\n",
       "        22939.48809524,  32133.96761905,  28474.9447619 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradiente(X,Y,theta):\n",
    "    return (X.T @ ((X @ theta)-Y))/N\n",
    "gradiente(X,Y,theta)\n",
    "\n",
    "#array([   990.01230159,   1043.94597798,  19450.57309524,  44620.83968254,\n",
    "#       179324.11406746,  69488.57619048,  37771.13047619, 100519.9381746 ,\n",
    "#        92454.55920635,  99401.69781746,  59131.64678571,  38349.26809524,\n",
    "#        22939.48809524,  32133.96761905,  28474.9447619 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#004D7F\"></i>\n",
    " </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## <font color=\"#004D7F\"> Gradiente descendiente </font>\n",
    "\n",
    "<br>\n",
    "A lo largo de los ejercicios anteriores se ha visto como el valor de la derivada parcial de la función de coste para un parámetro $\\theta_j$, $\\frac{\\partial}{\\partial \\theta_j}coste(\\theta)$, proporciona información útil sobre la ubicación del valor óptimo $\\theta_j$. Dado un valor cualquiera para $\\theta_j$, es posible sustituir dicho valor por otro  $\\theta_j'$ que esté más cerca del óptimo mediante la regla de actualización:\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\theta_j' = \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}coste(\\theta)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "Si la derivada parcial es positiva, entonces $\\theta_j'<\\theta_j$, es decir, el valor de $\\theta_j$ se desplaza hacia la izquierda (decrece). Si es negativa, entonces $\\theta_j'>\\theta_j$, es decir, el valor de $\\theta_j$ se desplaza hacia la derecha (crece). Por otra parte, el valor absoluto de la derivada determina la magnitud del cambio, de modo que cuando $\\theta_j$ esté cerca del óptimo, el cambio será menor. Si el valor de $\\alpha$ es suficientemente pequeño, el algoritmo converge al valor óptimo para $\\theta.$\n",
    "\n",
    "<br>\n",
    "El algoritmo de la gradiente descendiente se basa en establecer un conjunto de valores aleatorios para $\\theta$, y actualizarlos progresivamente mediante esta regla hasta que la disminución en la función de coste no supere un umbral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3><font color=\"#004D7F\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 7</font></h3>\n",
    "\n",
    "\n",
    "A partir de la configuración en la que $\\theta_j=0 \\,\\, \\forall j$, fijar $\\theta_7=10$ y ejecutar la regla de actualización 100 veces con un valor $\\alpha=0.00001$. Dibujar una gráfica en la que el eje X contenga lo valores de $\\theta_7$ y el eje Y el coste correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[397679.34138485 320431.04265494 258188.07316525]\n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros(15,)\n",
    "theta[7]=10\n",
    "\n",
    "iteraciones = 100\n",
    "alpha = 0.00001\n",
    "\n",
    "valores_7 = np.zeros(iteraciones)   # Almacenará los distintos valores de θ₇\n",
    "costes_7 = np.zeros(iteraciones)    # Almacenará los costes correspondientes.\n",
    "\n",
    "for it in range(iteraciones):\n",
    "    theta[7] = theta[7] - alpha * np.dot(((X@theta)-Y),X[:,7])/N\n",
    "    valores_7[it] = theta[7]\n",
    "    costes_7[it] = get_coste(X,Y,theta)\n",
    "    \n",
    "f, axes_grid = plt.subplots(1, 2)\n",
    "axes_grid[0].plot(range(iteraciones), costes_7)\n",
    "axes_grid[0].set_title(\"Coste \\ Iteración\")\n",
    "axes_grid[1].plot(range(iteraciones), valores_7, 'c')\n",
    "axes_grid[1].set_title(r\"$\\theta_7$ \\ Iteración\");\n",
    "\n",
    "print(costes_7[:3])   \n",
    "#[397679.34138485 320431.04265494 258188.07316525]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\"><font size=4> <i class=\"fa fa-check-square-o\" aria-hidden=\"true\" style=\"color:#004D7F\"></i>\n",
    " </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3><font color=\"#004D7F\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 8</font></h3>\n",
    "\n",
    "Implementar una función denominada `grad_descendiente(X,Y, alpha, iteraciones)` que utilice el algoritmo de gradiente descendiente para obtener los valores óptimos de los parámetros. Esta función debe devolver también la función de coste para cada iteración. (Aunque no es necesario, y con el fin de contrastar los resultados, inicializar el vector con $\\theta_j=0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descendiente(X,Y, alpha, iteraciones):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    costes = np.zeros(iteraciones)\n",
    "    for it in range(iteraciones):\n",
    "        theta = theta - alpha * gradiente(X,Y,theta)\n",
    "        costes[it] = get_coste(X, Y, theta)\n",
    "    return theta, costes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar la función fijando $\\alpha=0.000025$ e $iteraciones=100000$. Dibujar el coste por iteración para las 100 primeras iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12715009900295213\n"
     ]
    }
   ],
   "source": [
    "theta_gd, costes = grad_descendiente(X,Y, 0.000025, 100000)\n",
    "\n",
    "n_iteracion = np.arange(100)\n",
    "plt.plot(n_iteracion, costes[:100]);\n",
    "\n",
    "print(costes[-1])\n",
    "#0.12715009900295215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> \n",
    "&nbsp; Se aprecia que la función de coste desciende hasta que se estabiliza. Primero más rápido, y luego más lentamente.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "&nbsp; Por otra parte, si $\\alpha$ es demasiado grande, la función no converge. Esto se debe a que el cambio en la actualización es demasiado intenso, y en lugar de acercase al óptimo, puede pasar a un valor al otro lado, en el que la derivada tenga el signo distinto. Es decir, el algoritmo puede diverger si $\\alpha$ es demasiado grande.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimir los parámetros y costes devueltos por la ecuación normal, y los devueltos por el algoritmo gradiente descendiente. ¿Qué se observa? ¿Son iguales? ¿Qué ocurre si se aumenta el número de iteraciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros (ecuación normal):\n",
      "[-6.66969804e+00  1.08164715e+01  1.18560354e-03  2.09099106e-02\n",
      "  1.12116500e-02  1.54901224e-02  1.09636088e-01 -4.40300672e-03\n",
      "  4.58947733e-04 -5.61802302e-04 -1.01436163e-02  3.30465240e-02\n",
      "  1.01942061e-01  1.79469659e-02  7.37252454e-02]\n",
      "\n",
      "\n",
      "\n",
      "Parámetros (gradiente desdendiente)\n",
      "[ 4.80453012e-03  5.55196046e-03 -2.50626264e-02  2.15281642e-02\n",
      " -2.84353889e-03  3.34387156e-02  1.32111455e-01  9.46059628e-03\n",
      "  7.81580805e-05  2.72879430e-02 -7.49980868e-03  5.11114068e-02\n",
      "  1.13958025e-01  2.06157718e-02  8.24435865e-02]\n",
      "\n",
      "\n",
      "\n",
      "Coste con ecuación normal: 0.1233\n",
      "Coste con gradiente descenciente: 0.1272\n"
     ]
    }
   ],
   "source": [
    "theta_ne = get_lrmodel_ne(X,Y)\n",
    "\n",
    "print(\"Parámetros (ecuación normal):\")\n",
    "print(theta_ne)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Parámetros (gradiente desdendiente)\")\n",
    "print(theta_gd)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Coste con ecuación normal: {:.4f}\".format(get_coste(X,Y,theta_ne)))\n",
    "print(\"Coste con gradiente descenciente: {:.4f}\".format(get_coste(X,Y,theta_gd)))\n",
    "\n",
    "#Coste con ecuación normal: 0.1233\n",
    "#Coste con gradiente descenciente: 0.1272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "&nbsp; Aunque gradiente descendiente converge, lo hace lentamente. Sobre todo cuando se acerca al óptimo, y los costes no coinciden, aunque son muy parecidos\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# <font color=\"#004D7F\"> Normalización </font>\n",
    "\n",
    "<br>\n",
    "En el ejercicio anterior se puede apreciar que, si bien los parámetros devueltos por el método gradiente descendiente son parecidos a los devueltos por la ecuación normal, y el coste es reducido, el aumento en el número de iteraciones no conduce a una convergencia total.\n",
    "\n",
    "<br> \n",
    "Para acelerar la convergencia del algoritmo, las características se deben normalizar de modo que $\\bar{X_j}=0$ y $\\sigma{X_j}=1$, es decir, que los valores de cada variable $X_j$ pasen a tener media 0 y desviación 1. Para ello, se aplica la siguiente expresión.\n",
    "\n",
    "$$\n",
    "x_j^{(i)} = \\frac{x_j^{(i)}-\\bar{X_j}}{\\sigma(X_j)}\n",
    "$$\n",
    "\n",
    "El algoritmo de la gradiente descendiente devuelve los coeficientes $\\alpha$ que minimizan el coste para las variables normalizadas. Por tanto, hay transformarlos para su aplicación sobre las variables en los rangos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80164696, -0.82024615, -1.74007329, -0.84124583, -0.65620473],\n",
       "       [ 1.56506057, -1.56257271, -1.81958344, -0.19346239,  0.57478975]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se normaliza la base de datos\n",
    "X_n = X[:,1:]\n",
    "mediaX = np.mean(X_n, axis=0)    # Media de cada columna\n",
    "stdx = np.std(X_n, axis = 0)     # Desviación de cada columna\n",
    "X_n = (X_n-mediaX)/stdx\n",
    "X_n[:2,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.65204025e+00  1.08004454e+01  1.14844176e-03  2.09104204e-02\n",
      "  1.12125312e-02  1.54897407e-02  1.09635810e-01 -4.40192879e-03\n",
      "  4.57494287e-04 -5.60798830e-04 -1.01448369e-02  3.30460215e-02\n",
      "  1.01938325e-01  1.79442124e-02  7.37259471e-02]\n",
      "Coste con gradiente descendiente:  0.12325890328133926\n"
     ]
    }
   ],
   "source": [
    "# No se copia la primera columna, porque el cálculo de la desviación daría error.\n",
    "X_n = X[:,1:]    \n",
    "\n",
    "# Se normaliza la base de datos\n",
    "mediaX = np.mean(X_n, axis=0)     # Media de cada columna\n",
    "stdX = np.std(X_n, axis = 0)      # Desviación de cada columna\n",
    "X_n = (X_n-mediaX)/stdX\n",
    "\n",
    "# Se añade la columna de unos.\n",
    "X_n = np.hstack([np.ones((X_n.shape[0],1)), X_n]) \n",
    "\n",
    "# Gradiente descendiente sobre la base de datos normalizada\n",
    "theta, _ = grad_descendiente(X_n,Y, 0.05, 10000)\n",
    "\n",
    "# Se transforman los coeficientes.\n",
    "theta[1:] = theta[1:] / stdX\n",
    "theta[0] -= theta[1:].dot(mediaX)\n",
    "\n",
    "# Imprime los resultados\n",
    "print(theta)\n",
    "print(\"Coste con gradiente descendiente: \",get_coste(X,Y,theta))\n",
    "#Coste con gradiente descendiente:  0.12839886948374146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> \n",
    "&nbsp; Ahora los resultados son más similares a los devueltos por la ecuación normal.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: right\"> <font size=5> [<i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\">]</i></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
