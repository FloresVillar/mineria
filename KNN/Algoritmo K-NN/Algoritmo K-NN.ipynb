{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7db550-270c-445f-8035-806ea20b4068",
   "metadata": {},
   "source": [
    "# <img src=\"uni-logo.png\" alt=\"Logo UNI\" width=150 hight=300 align=\"right\">\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "<h1><font color=\"#7F000E\" size=4>Minería de Datos (CC442)</font></h1>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#7F000E\" size=6>Teoría: Algoritmo K-Nearest Neighbors (KNN) </font></h1>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#7F000E\" size=3>Yuri Coicca, M.Sc.</font><br>\n",
    "<font color=\"#7F000E\" size=3>Facultad de Ciencias</font><br>\n",
    "<font color=\"#7F000E\" size=3>Ciencia de la Computación - UNI</font><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f8d02-7eaf-49f7-81f4-fbce62c19e1b",
   "metadata": {},
   "source": [
    "El algoritmo **K-Nearest Neighbors (K-NN)** o **K-Vecinos más Cercanos** es uno de los algoritmos de aprendizaje supervisado más simples e intuitivos. A diferencia de otros modelos, K-NN es un algoritmo de **aprendizaje perezoso (lazy learning)**, lo que significa que no tiene una fase de entrenamiento real; simplemente \"memoriza\" los datos y realiza los cálculos al momento de la predicción.\n",
    "\n",
    "Aquí tienes la explicación detallada, el pseudocódigo y su implementación.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ¿Cómo funciona K-NN? (Pasos del algoritmo)\n",
    "\n",
    "1.  **Elegir el valor de $K$**: Decidir cuántos vecinos cercanos se consultarán (por ejemplo, $K=3$ o $K=5$).\n",
    "2.  **Calcular la distancia**: Para un nuevo punto (dato de prueba), calcular la distancia entre ese punto y **todos** los demás puntos del conjunto de entrenamiento.\n",
    "    *   *Distancia Euclidiana* (la más común): $d = \\sqrt{\\sum (x_i - y_i)^2}$\n",
    "3.  **Seleccionar los vecinos**: Ordenar las distancias de menor a mayor y seleccionar los $K$ puntos más cercanos.\n",
    "4.  **Realizar la predicción**:\n",
    "    *   **Clasificación**: Se hace una votación mayoritaria. El nuevo punto pertenece a la clase más frecuente entre sus $K$ vecinos.\n",
    "    *   **Regresión**: Se calcula el promedio de los valores de los $K$ vecinos.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Pseudocódigo (Lógica pura)\n",
    "\n",
    "```text\n",
    "Entrada: DatosEntrenamiento (X, y), PuntoConsulta (q), Valor K\n",
    "\n",
    "1. Para cada punto 'p' en DatosEntrenamiento:\n",
    "    a. Calcular distancia D entre 'p' y 'q' (ej. Euclidiana)\n",
    "    b. Guardar (D, etiqueta_de_p) en una lista\n",
    "\n",
    "2. Ordenar la lista de distancias de menor a mayor.\n",
    "\n",
    "3. Tomar los primeros K elementos de la lista ordenada.\n",
    "\n",
    "4. Si el problema es Clasificación:\n",
    "    Retornar la clase más frecuente (moda) de los K elementos.\n",
    "   Si el problema es Regresión:\n",
    "    Retornar el promedio (media) de los valores de los K elementos.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Implementación Manual en Python (Desde cero)\n",
    "\n",
    "Esta versión ayuda a entender la mecánica interna antes de usar librerías:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def distancia_euclidiana(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # En KNN, el entrenamiento es solo guardar los datos\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicciones = [self._predecir_uno(x) for x in X_test]\n",
    "        return np.array(predicciones)\n",
    "\n",
    "    def _predecir_uno(self, x_consulta):\n",
    "        # 1. Calcular distancias contra todos los puntos de entrenamiento\n",
    "        distancias = [distancia_euclidiana(x_consulta, x_train) for x_train in self.X_train]\n",
    "        \n",
    "        # 2. Obtener los índices de los K vecinos más cercanos\n",
    "        indices_k_vecinos = np.argsort(distancias)[:self.k]\n",
    "        \n",
    "        # 3. Extraer las etiquetas de esos vecinos\n",
    "        etiquetas_vecinos = [self.y_train[i] for i in indices_k_vecinos]\n",
    "        \n",
    "        # 4. Votación mayoritaria\n",
    "        votos = Counter(etiquetas_vecinos).most_common(1)\n",
    "        return votos[0][0]\n",
    "\n",
    "# Ejemplo de uso\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 3], [6, 5], [7, 8]])\n",
    "y_train = np.array([0, 0, 0, 1, 1]) # Clase 0 y Clase 1\n",
    "\n",
    "clf = KNN(k=3)\n",
    "clf.fit(X_train, y_train)\n",
    "prediccion = clf.predict([[4, 4]])\n",
    "print(f\"El punto [4, 4] pertenece a la clase: {prediccion}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Relación con KD-Tree y Ball-Tree (Optimización)\n",
    "Es importante notar que el paso 2 de K-NN (calcular la distancia contra **todos** los puntos) tiene una complejidad $O(n)$, lo cual es muy lento si tienes millones de datos.\n",
    "\n",
    "*   Si usas **fuerza bruta**, K-NN compara el punto contra todo el dataset.\n",
    "*   Si configuras K-NN para usar un **KD-Tree** o **Ball-Tree**, el algoritmo ya no compara contra todos, sino que navega por el árbol para encontrar los $K$ vecinos en tiempo $O(\\log n)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Consideraciones Importantes\n",
    "1.  **Escalado de datos (Feature Scaling)**: K-NN depende totalmente de las distancias. Si una variable está en el rango [0-1] y otra en [0-1000], la segunda dominará la distancia. Siempre hay que **normalizar o estandarizar** los datos.\n",
    "2.  **Elección de K**:\n",
    "    *   Un $K$ muy pequeño (ej. $K=1$) es sensible al ruido (overfitting).\n",
    "    *   Un $K$ muy grande suaviza demasiado las fronteras (underfitting).\n",
    "    *   Suele elegirse un número impar para evitar empates en la votación.\n",
    "3.  **Maldición de la dimensionalidad**: En espacios de muchas dimensiones, todos los puntos parecen estar \"lejos\" de todos, y K-NN pierde efectividad (aquí es donde estructuras como Ball-Tree ayudan, pero no hacen milagros).\n",
    "\n",
    "### Implementación Profesional (Scikit-learn)\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Escalar datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Crear y entrenar modelo (aquí puedes elegir 'kd_tree' o 'ball_tree')\n",
    "model = KNeighborsClassifier(n_neighbors=3, algorithm='auto') \n",
    "model.fit(X_scaled, y_train)\n",
    "\n",
    "# Predecir\n",
    "pred = model.predict(scaler.transform([[4, 4]]))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
