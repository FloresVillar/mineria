{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46c9428-2a9f-4440-94a1-cf545cfc9da0",
   "metadata": {},
   "source": [
    "# <img src=\"uni-logo.png\" alt=\"Logo UNI\" width=150 hight=300 align=\"right\">\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "<h1><font color=\"#7F000E\" size=4>Minería de Datos (CC442)</font></h1>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#7F000E\" size=6>Teoría: Algoritmo del KD-Tree </font></h1>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#7F000E\" size=3>Yuri Coicca, M.Sc.</font><br>\n",
    "<font color=\"#7F000E\" size=3>Facultad de Ciencias</font><br>\n",
    "<font color=\"#7F000E\" size=3>Ciencia de la Computación - UNI</font><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277e916-db73-4c99-87e4-d63b7f391871",
   "metadata": {},
   "source": [
    "El **KD-Tree** (K-Dimensional Tree) es una estructura de datos de tipo árbol binario utilizada en **Machine Learning** para organizar puntos en un espacio de $K$ dimensiones. Su objetivo principal es optimizar la búsqueda de vecinos cercanos (**K-Nearest Neighbors**), reduciendo la complejidad de una búsqueda lineal $O(n)$ a una búsqueda logarítmica $O(\\log n)$ en promedio.\n",
    "\n",
    "A continuación, te explico cómo funciona el algoritmo de construcción y el de búsqueda.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Construcción del KD-Tree\n",
    "El árbol se construye dividiendo recursivamente el conjunto de datos en dos mitades usando hiperplanos perpendiculares a los ejes de las coordenadas.\n",
    "\n",
    "#### Pasos del algoritmo:\n",
    "1.  **Elegir dimensión:** Se selecciona un eje (dimensión) para dividir los datos. Una forma común es alternar el eje según la profundidad del árbol: `eje = profundidad % k`.\n",
    "2.  **Encontrar la mediana:** Se ordena el conjunto de puntos respecto al eje elegido y se selecciona el punto que representa la **mediana**.\n",
    "3.  **Crear nodo:** La mediana se convierte en el nodo actual (raíz del subárbol).\n",
    "4.  **Dividir y repetir:** \n",
    "    *   Los puntos con valores menores a la mediana van al **subárbol izquierdo**.\n",
    "    *   Los puntos con valores mayores van al **subárbol derecho**.\n",
    "5.  Se repite el proceso recursivamente hasta que no queden puntos.\n",
    "\n",
    "#### Pseudocódigo (Construcción):\n",
    "```python\n",
    "def construir_kdtree(puntos, profundidad=0):\n",
    "    if not puntos:\n",
    "        return None\n",
    "    \n",
    "    k = len(puntos[0]) # Número de dimensiones\n",
    "    eje = profundidad % k\n",
    "    \n",
    "    # Ordenar y elegir mediana\n",
    "    puntos.sort(key=lambda x: x[eje])\n",
    "    indice_mediana = len(puntos) // 2\n",
    "    \n",
    "    # Crear nodo y construir subárboles\n",
    "    return {\n",
    "        'punto': puntos[indice_mediana],\n",
    "        'izquierdo': construir_kdtree(puntos[:indice_mediana], profundidad + 1),\n",
    "        'derecho': construir_kdtree(puntos[indice_mediana + 1:], profundidad + 1)\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Búsqueda del Vecino más Cercano (Nearest Neighbor)\n",
    "La búsqueda es eficiente porque permite descartar regiones enteras del espacio sin tener que calcular la distancia de cada punto.\n",
    "\n",
    "#### Pasos del algoritmo:\n",
    "1.  **Descenso rápido:** Se baja por el árbol desde la raíz como si se estuviera insertando el punto de consulta, hasta llegar a una hoja.\n",
    "2.  **Actualizar mejor candidato:** El punto en la hoja se marca inicialmente como el \"mejor vecino\" actual.\n",
    "3.  **Backtracking (Retroceso):** Se sube por el árbol evaluando cada nodo padre:\n",
    "    *   Si la distancia al padre es menor que la del \"mejor vecino\", se actualiza.\n",
    "    *   **Poda (Crucial):** Se comprueba si existe la posibilidad de que haya puntos mejores en el *otro lado* del hiperplano de división. Si la distancia perpendicular desde el punto de consulta al hiperplano es menor que la distancia al \"mejor vecino\" actual, entonces **se debe explorar la otra rama**.\n",
    "4.  El proceso termina al regresar a la raíz.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Ventajas y Desventajas\n",
    "\n",
    "| Ventaja | Desventaja |\n",
    "| :--- | :--- |\n",
    "| **Velocidad:** En dimensiones bajas ($< 20$), la búsqueda es extremadamente rápida $O(\\log n)$. | **Maldición de la dimensionalidad:** En dimensiones muy altas, el algoritmo termina revisando casi todas las ramas, volviéndose tan lento como la búsqueda lineal. |\n",
    "| **Memoria:** Estructura compacta que organiza el espacio de forma eficiente. | **Datos estáticos:** Es costoso reequilibrar el árbol si se añaden o eliminan puntos constantemente. |\n",
    "\n",
    "### Implementación en Python\n",
    "En la práctica, no necesitas programarlo desde cero. La librería **Scikit-learn** lo tiene optimizado:\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "\n",
    "# Datos de ejemplo (2D)\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [8, 8]])\n",
    "\n",
    "# 1. Construir el árbol\n",
    "tree = KDTree(X, leaf_size=2)\n",
    "\n",
    "# 2. Buscar el vecino más cercano a un punto nuevo\n",
    "dist, ind = tree.query([[2, 3]], k=1)\n",
    "\n",
    "print(f\"Vecino más cercano: {X[ind[0]]}, Distancia: {dist[0]}\")\n",
    "```\n",
    "\n",
    "**¿Cuándo usarlo?** Es ideal para algoritmos de **KNN** o **clustering** (como Mean Shift) cuando tienes muchos datos pero pocas características (features). Si tienes cientos de dimensiones, es mejor usar alternativas como **Ball Trees** o **LSH** (Locality Sensitive Hashing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
